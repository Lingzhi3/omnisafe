# Copyright 2019 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Compute the exploitability of a bot / strategy in a 2p sequential game.

This computes the value that a policy achieves against a worst-case opponent.
The policy applies to both player 1 and player 2, and hence we have a 2-player
symmetric zero-sum game, so the game value is zero for both players, and hence
value-vs-best-response is equal to exploitability.

We construct information sets, each consisting of a list of (state, probability)
pairs where probability is a counterfactual reach probability, i.e. the
probability that the state would be reached if the best responder (the current
player) played to reach it. This is the product of the probabilities of the
necessary chance events and opponent action choices required to reach the node.

These probabilities give us the correct weighting for possible states of the
world when considering our best response for a particular information set.

The values we calculate are values of being in the specific state. Unlike in a
CFR algorithm, they are not weighted by reach probabilities. These values
take into account the whole state, so they may depend on information which is
unknown to the best-responding player.

计算一个机器人/策略在一个两人顺序博弈中的可利用性。

这个计算方法可以计算一个策略在最坏情况下的表现。该策略适用于玩家1和玩家2，因此我们有一个两人对称的零和博弈，因此对于两个玩家来说，游戏价值都是零，因此价值与最佳响应相等，即可利用性。

我们构建信息集，每个信息集由一个（状态，概率）对列表组成，其中概率是反事实到达概率，即如果最佳响应者（当前玩家）采取行动到达该状态的概率。这是达到该节点所需的必要机会事件和对手行动选择的概率的乘积。

这些概率为我们提供了在考虑特定信息集的最佳响应时可能的世界状态的正确加权。

我们计算的值是处于特定状态的值。与CFR算法不同，它们没有按到达概率加权。这些值考虑了整个状态，因此可能取决于最佳响应玩家不知道的信息。
"""

import collections

import numpy as np

from open_spiel.python import policy as policy_lib
from open_spiel.python.algorithms import best_response as pyspiel_best_response
import pyspiel


def _state_values(state, num_players, policy):
  """Value of a state for every player given a policy."""
  if state.is_terminal():
    return np.array(state.returns())
  else:
    if state.is_simultaneous_node():
      p_action = tuple(policy_lib.joint_action_probabilities(state, policy))

    else:
      p_action = (
          state.chance_outcomes()
          if state.is_chance_node()
          else policy.action_probabilities(state).items()
      )
    return sum(
        prob
        * _state_values(policy_lib.child(state, action), num_players, policy)
        for action, prob in p_action
    )


def best_response(game, policy, player_id):
  """Returns information about the specified player's best response.

  Given a game and a policy for every player, computes for a single player their
  best unilateral strategy. Returns the value improvement that player would
  get, the action they should take in each information state, and the value
  of each state when following their unilateral policy.

  Args:
    game: An open_spiel game, e.g. kuhn_poker
    policy: A `policy.Policy` object. This policy should depend only on the
      information state available to the current player, but this is not
      enforced.
    player_id: The integer id of a player in the game for whom the best response
      will be computed.

  Returns:
    A dictionary of values, with keys:
      best_response_action: The best unilateral strategy for `player_id` as a
        map from infostatekey to action_id.
      best_response_state_value: The value obtained for `player_id` when
        unilaterally switching strategy, for each state.
      best_response_value: The value obtained for `player_id` when unilaterally
        switching strategy.
      info_sets: A dict of info sets, mapping info state key to a list of
        `(state, counterfactual_reach_prob)` pairs.
      nash_conv: `best_response_value - on_policy_value`
      on_policy_value: The value for `player_id` when all players follow the
        policy
      on_policy_values: The value for each player when all players follow the
        policy

    该函数返回指定玩家的最佳响应信息。

    给定一个游戏和每个玩家的策略，计算单个玩家的最佳单边策略。返回该玩家可以获得的价值提升、他们应该在每个信息状态下采取的行动以及在遵循他们的单边策略时每个状态的价值。

    参数：
    - game: 一个open_spiel游戏，例如kuhn_poker。
    - policy: 一个`policy.Policy`对象。该策略应仅依赖于当前玩家可用的信息状态，但这并没有被强制执行。
    - player_id: 要计算最佳响应的玩家在游戏中的整数ID。

    返回值：
    - 一个字典，包含以下键值对：
      - best_response_action: `player_id`的最佳单边策略，作为从infostatekey到action_id的映射。
      - best_response_state_value: 在单方面切换策略时，对于每个状态，`player_id`获得的价值。
      - best_response_value: 在单方面切换策略时，`player_id`获得的价值。
      - info_sets: 信息集的字典，将信息状态键映射到`(state, counterfactual_reach_prob)`对的列表。
      - nash_conv: `best_response_value - on_policy_value`。
      - on_policy_value: 当所有玩家遵循策略时，`player_id`的价值。
      - on_policy_values: 当所有玩家遵循策略时，每个玩家的价值。

    其中，infostatekey是信息状态的唯一标识符，counterfactual_reach_prob是反事实到达概率，表示在当前玩家采取最佳响应策略的情况下，到达该信息状态的概率。nash_conv是可利用性，表示最佳响应策略相对于当前策略的改进。
  """
  root_state = game.new_initial_state()
  br = pyspiel_best_response.BestResponsePolicy(game, player_id, policy,
                                                root_state)
  on_policy_values = _state_values(root_state, game.num_players(), policy)
  best_response_value = br.value(root_state)

  # Get best response action for unvisited states
  for infostate in set(br.infosets) - set(br.cache_best_response_action):
    br.best_response_action(infostate)

  return {
      "best_response_action": br.cache_best_response_action,
      "best_response_state_value": br.cache_value,
      "best_response_value": best_response_value,
      "info_sets": br.infosets,
      "nash_conv": best_response_value - on_policy_values[player_id],
      "on_policy_value": on_policy_values[player_id],
      "on_policy_values": on_policy_values,
  }


def exploitability(game, policy):
  """Returns the exploitability of the policy in the game.

  This is implemented only for 2 players constant-sum games, and is equivalent
  to NashConv / num_players in that case. Prefer using `nash_conv`.

  Args:
    game: An open_spiel game, e.g. kuhn_poker
    policy: A `policy.Policy` object. This policy should depend only on the
      information state available to the current player, but this is not
      enforced.

  Returns:
    The value that this policy achieves when playing against the worst-case
    non-cheating opponent, averaged across both starting positions. It has a
    minimum of zero (assuming the supplied policy is non-cheating) and
    this bound is achievable in a 2p game.

  Raises:
    ValueError if the game is not a two-player constant-sum turn-based game.
    该函数返回游戏中策略的可利用性。

    该函数仅适用于两人常和博弈，并且在这种情况下等价于NashConv / num_players。建议使用`nash_conv`。

    参数：
    - game: 一个open_spiel游戏，例如kuhn_poker。
    - policy: 一个`policy.Policy`对象。该策略应仅依赖于当前玩家可用的信息状态但这并没有被强制执行。

    返回值：
    - 当与最坏情况下的非作弊对手对战时，该策略在两个起始位置上平均获得的价值。它的最小值为零（假设提供的策略是非作弊的），在2p游戏中可以达到这个界限。

    如果游戏不是两人常和回合制游戏，则会引发ValueError。
  """
  if game.num_players() != 2:
    raise ValueError("Game must be a 2-player game")
  game_info = game.get_type()
  if game_info.dynamics != pyspiel.GameType.Dynamics.SEQUENTIAL:
    raise ValueError("The game must be turn-based, not {}".format(
        game_info.dynamics))
  if game_info.utility not in (pyspiel.GameType.Utility.ZERO_SUM,
                               pyspiel.GameType.Utility.CONSTANT_SUM):
    raise ValueError("The game must be constant- or zero-sum, not {}".format(
        game_info.utility))
  root_state = game.new_initial_state()
  nash_conv_value = (
      sum(
          pyspiel_best_response.CPPBestResponsePolicy(
              game, best_responder, policy).value(root_state)
          for best_responder in range(game.num_players())) - game.utility_sum())
  return nash_conv_value / game.num_players()


_NashConvReturn = collections.namedtuple("_NashConvReturn",
                                         ["nash_conv", "player_improvements"])


def nash_conv(game, policy, return_only_nash_conv=True, use_cpp_br=False):
  r"""Returns a measure of closeness to Nash for a policy in the game.

  See https://arxiv.org/pdf/1711.00832.pdf for the NashConv definition.

  Args:
    game: An open_spiel game, e.g. kuhn_poker
    policy: A `policy.Policy` object. This policy should depend only on the
      information state available to the current player, but this is not
      enforced.
    return_only_nash_conv: Whether to only return the NashConv value, or a
      namedtuple containing additional statistics. Prefer using `False`, as we
      hope to change the default to that value.
    use_cpp_br: if True, compute the best response in c++

  Returns:
    Returns a object with the following attributes:
    - player_improvements: A `[num_players]` numpy array of the improvement
      for players (i.e. value_player_p_versus_BR - value_player_p).
    - nash_conv: The sum over all players of the improvements in value that each
      player could obtain by unilaterally changing their strategy, i.e.
      sum(player_improvements).

    该函数计算策略的可利用性和最佳响应策略。

    参数：
    - game: 一个open_spiel游戏，例如kuhn_poker。
    - policy: 一个`policy.Policy`对象。该策略应仅依赖于当前玩家可用的信息状态但这没有被强制执行。
    - return_only_nash_conv: 是否仅返回NashConv值，或包含其他统计信息的命名元组。建议使用`False`，因为我们希望将默认值更改为该值。
    - use_cpp_br: 如果为True，则使用c++计算最佳响应。

    返回值：
    - 返回一个对象，具有以下属性：
      - player_improvements: 一个`[num_players]`的numpy数组，表示每个玩家的改进（即value_player_p_versus_BR - value_player_p）。
      - nash_conv: 每个玩家可以通过单方面改变其策略获得的价值改进的总和，即sum(player_improvements)。
  """
  root_state = game.new_initial_state()
  if use_cpp_br:
    best_response_values = np.array([
        pyspiel_best_response.CPPBestResponsePolicy(
            game, best_responder, policy).value(root_state)
        for best_responder in range(game.num_players())
    ])
  else:
    best_response_values = np.array([
        pyspiel_best_response.BestResponsePolicy(
            game, best_responder, policy).value(root_state)
        for best_responder in range(game.num_players())
    ])
  on_policy_values = _state_values(root_state, game.num_players(), policy)      # _state_values计算当前策略在给定状态下的值
  player_improvements = best_response_values - on_policy_values
  nash_conv_ = sum(player_improvements)
  if return_only_nash_conv:
    return nash_conv_
  else:
    return _NashConvReturn(
        nash_conv=nash_conv_, player_improvements=player_improvements)
